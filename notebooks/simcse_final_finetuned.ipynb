{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IYo1icILUGPk"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OVaZC9KTUHFm"
      },
      "outputs": [],
      "source": [
        "class ImprovedSimCSEDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128, use_hard_negatives=True):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.use_hard_negatives = use_hard_negatives\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        positive_text = self.augment_text(text)\n",
        "\n",
        "        encoding_orig = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        encoding_pos = self.tokenizer(\n",
        "            positive_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids_orig': encoding_orig['input_ids'].flatten(),\n",
        "            'attention_mask_orig': encoding_orig['attention_mask'].flatten(),\n",
        "            'input_ids_pos': encoding_pos['input_ids'].flatten(),\n",
        "            'attention_mask_pos': encoding_pos['attention_mask'].flatten(),\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "    def augment_text(self, text):\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ilbyuspcUJkg"
      },
      "outputs": [],
      "source": [
        "class SimCSEModel(nn.Module):\n",
        "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                 pooling='mean', projection_dim=None, temperature=0.05):\n",
        "        super(SimCSEModel, self).__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.pooling = pooling\n",
        "        self.temperature = temperature\n",
        "        self.hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "        if projection_dim:\n",
        "            self.projection = nn.Sequential(\n",
        "                nn.Linear(self.hidden_size, projection_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(projection_dim, projection_dim)\n",
        "            )\n",
        "            self.output_dim = projection_dim\n",
        "        else:\n",
        "            self.projection = None\n",
        "            self.output_dim = self.hidden_size\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.15)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, return_embeddings=False, dropout_mask=None):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        if self.pooling == 'cls':\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        elif self.pooling == 'mean':\n",
        "            last_hidden = outputs.last_hidden_state\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden * mask_expanded, 1)\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "            embeddings = sum_embeddings / sum_mask\n",
        "        elif self.pooling == 'max':\n",
        "            last_hidden = outputs.last_hidden_state\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "            last_hidden[mask_expanded == 0] = -1e9\n",
        "            embeddings = torch.max(last_hidden, 1)[0]\n",
        "\n",
        "        if self.projection:\n",
        "            embeddings = self.projection(embeddings)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return embeddings\n",
        "\n",
        "        if dropout_mask is None:\n",
        "            dropout_mask = torch.randint(0, 3, (embeddings.shape[0],))\n",
        "\n",
        "        dropout_embeddings = embeddings.clone()\n",
        "        for i, mask in enumerate(dropout_mask):\n",
        "            if mask == 0:\n",
        "                dropout_embeddings[i] = self.dropout1(embeddings[i])\n",
        "            elif mask == 1:\n",
        "                dropout_embeddings[i] = self.dropout2(embeddings[i])\n",
        "            else:\n",
        "                dropout_embeddings[i] = self.dropout3(embeddings[i])\n",
        "\n",
        "        return dropout_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JYGI2O5qUL7k"
      },
      "outputs": [],
      "source": [
        "class SimCSEModel(nn.Module):\n",
        "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                 pooling='mean', projection_dim=None, temperature=0.05):\n",
        "        super(SimCSEModel, self).__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.pooling = pooling\n",
        "        self.temperature = temperature\n",
        "        self.hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "        if projection_dim:\n",
        "            self.projection = nn.Sequential(\n",
        "                nn.Linear(self.hidden_size, projection_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(projection_dim, projection_dim)\n",
        "            )\n",
        "            self.output_dim = projection_dim\n",
        "        else:\n",
        "            self.projection = None\n",
        "            self.output_dim = self.hidden_size\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.15)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, return_embeddings=False, dropout_mask=None):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        if self.pooling == 'cls':\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        elif self.pooling == 'mean':\n",
        "            last_hidden = outputs.last_hidden_state\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden * mask_expanded, 1)\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "            embeddings = sum_embeddings / sum_mask\n",
        "        elif self.pooling == 'max':\n",
        "            last_hidden = outputs.last_hidden_state\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "            last_hidden[mask_expanded == 0] = -1e9\n",
        "            embeddings = torch.max(last_hidden, 1)[0]\n",
        "\n",
        "        if self.projection:\n",
        "            embeddings = self.projection(embeddings)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return embeddings\n",
        "\n",
        "        if dropout_mask is None:\n",
        "            dropout_mask = torch.randint(0, 3, (embeddings.shape[0],))\n",
        "\n",
        "        dropout_embeddings = embeddings.clone()\n",
        "        for i, mask in enumerate(dropout_mask):\n",
        "            if mask == 0:\n",
        "                dropout_embeddings[i] = self.dropout1(embeddings[i])\n",
        "            elif mask == 1:\n",
        "                dropout_embeddings[i] = self.dropout2(embeddings[i])\n",
        "            else:\n",
        "                dropout_embeddings[i] = self.dropout3(embeddings[i])\n",
        "\n",
        "        return dropout_embeddings\n",
        "\n",
        "class SimCSETrainer:\n",
        "\n",
        "    def __init__(self, model, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.temperature = model.temperature\n",
        "\n",
        "    def compute_advanced_contrastive_loss(self, z1, z2, hard_negatives_weight=0.5):\n",
        "        batch_size = z1.shape[0]\n",
        "\n",
        "        z1 = F.normalize(z1, dim=1)\n",
        "        z2 = F.normalize(z2, dim=1)\n",
        "\n",
        "        sim_matrix = torch.matmul(z1, z2.T) / self.temperature\n",
        "\n",
        "        labels = torch.arange(batch_size).to(self.device)\n",
        "\n",
        "        loss_12 = F.cross_entropy(sim_matrix, labels)\n",
        "        loss_21 = F.cross_entropy(sim_matrix.T, labels)\n",
        "        loss = (loss_12 + loss_21) / 2\n",
        "\n",
        "        if hard_negatives_weight > 0:\n",
        "            mask = torch.eye(batch_size, device=self.device).bool()\n",
        "            neg_sim = sim_matrix.masked_fill(mask, -float('inf'))\n",
        "\n",
        "            k = min(5, batch_size - 1)\n",
        "            hard_neg_sim, _ = torch.topk(neg_sim, k, dim=1)\n",
        "\n",
        "            pos_sim = torch.diag(sim_matrix).unsqueeze(1)\n",
        "            hard_neg_loss = -torch.log(\n",
        "                torch.exp(pos_sim) / (torch.exp(pos_sim) + torch.exp(hard_neg_sim).sum(dim=1, keepdim=True))\n",
        "            ).mean()\n",
        "\n",
        "            loss = loss + hard_negatives_weight * hard_neg_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train_epoch(self, dataloader, optimizer, scheduler, use_mixup=False):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
        "            input_ids_orig = batch['input_ids_orig'].to(self.device)\n",
        "            attention_mask_orig = batch['attention_mask_orig'].to(self.device)\n",
        "            input_ids_pos = batch['input_ids_pos'].to(self.device)\n",
        "            attention_mask_pos = batch['attention_mask_pos'].to(self.device)\n",
        "\n",
        "            batch_size = input_ids_orig.shape[0]\n",
        "\n",
        "            dropout_mask1 = torch.randint(0, 3, (batch_size,))\n",
        "            dropout_mask2 = torch.randint(0, 3, (batch_size,))\n",
        "\n",
        "            z1 = self.model(input_ids_orig, attention_mask_orig, dropout_mask=dropout_mask1)\n",
        "            z2 = self.model(input_ids_pos, attention_mask_pos, dropout_mask=dropout_mask2)\n",
        "\n",
        "            if use_mixup and random.random() < 0.3:\n",
        "                lam = np.random.beta(0.2, 0.2)\n",
        "                indices = torch.randperm(batch_size)\n",
        "                z1 = lam * z1 + (1 - lam) * z1[indices]\n",
        "                z2 = lam * z2 + (1 - lam) * z2[indices]\n",
        "\n",
        "            loss = self.compute_advanced_contrastive_loss(z1, z2)\n",
        "\n",
        "            l2_reg = 0.01 * sum(p.pow(2.0).sum() for p in self.model.parameters())\n",
        "            loss = loss + l2_reg\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def train(self, train_dataloader, val_dataloader, epochs=5, learning_rate=2e-5, warmup_ratio=0.1):\n",
        "\n",
        "        encoder_param_ids = set(id(p) for p in self.model.encoder.parameters())\n",
        "        encoder_params = []\n",
        "        other_params = []\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            if id(p) in encoder_param_ids:\n",
        "                encoder_params.append(p)\n",
        "            else:\n",
        "                other_params.append(p)\n",
        "\n",
        "        optimizer = AdamW([\n",
        "            {'params': encoder_params, 'lr': learning_rate * 0.1},\n",
        "            {'params': other_params, 'lr': learning_rate}\n",
        "        ], weight_decay=0.01)\n",
        "\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "        warmup_steps = int(warmup_ratio * total_steps)\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience = 3\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            use_mixup = epoch >= 2\n",
        "            train_loss = self.train_epoch(train_dataloader, optimizer, scheduler, use_mixup)\n",
        "\n",
        "            val_loss = self.evaluate(val_dataloader)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                self.save_model(\"best_improved_simcse_model\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    break\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "                input_ids_orig = batch['input_ids_orig'].to(self.device)\n",
        "                attention_mask_orig = batch['attention_mask_orig'].to(self.device)\n",
        "                input_ids_pos = batch['input_ids_pos'].to(self.device)\n",
        "                attention_mask_pos = batch['attention_mask_pos'].to(self.device)\n",
        "                z1 = self.model(input_ids_orig, attention_mask_orig, return_embeddings=True)\n",
        "                z2 = self.model(input_ids_pos, attention_mask_pos, return_embeddings=True)\n",
        "\n",
        "                loss = self.compute_advanced_contrastive_loss(z1, z2, hard_negatives_weight=0)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def generate_embeddings(self, texts, batch_size=32):\n",
        "        self.model.eval()\n",
        "        embeddings = []\n",
        "\n",
        "        dataset = ImprovedSimCSEDataset(texts, self.tokenizer, use_hard_negatives=False)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
        "                input_ids = batch['input_ids_orig'].to(self.device)\n",
        "                attention_mask = batch['attention_mask_orig'].to(self.device)\n",
        "\n",
        "                batch_embeddings_list = []\n",
        "                for _ in range(3):\n",
        "                    batch_embeddings = self.model(input_ids, attention_mask, return_embeddings=True)\n",
        "                    batch_embeddings_list.append(batch_embeddings)\n",
        "\n",
        "                final_embeddings = torch.stack(batch_embeddings_list).mean(dim=0)\n",
        "                embeddings.append(final_embeddings.cpu().numpy())\n",
        "\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        torch.save(self.model.state_dict(), os.path.join(path, \"model.pt\"))\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "\n",
        "        config = {\n",
        "            'pooling': self.model.pooling,\n",
        "            'output_dim': self.model.output_dim,\n",
        "            'temperature': self.temperature\n",
        "        }\n",
        "        torch.save(config, os.path.join(path, \"config.pt\"))\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model.load_state_dict(torch.load(os.path.join(path, \"model.pt\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ij4muasgURLe"
      },
      "outputs": [],
      "source": [
        "def load_data(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    texts = df['text'].dropna().astype(str).tolist()\n",
        "\n",
        "    texts = [text.strip() for text in texts if text.strip()]\n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Igz6grITUUM-"
      },
      "outputs": [],
      "source": [
        "def post_process_embeddings(embeddings, method='pca', n_components=256):\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    if method == 'pca':\n",
        "        scaler = StandardScaler()\n",
        "        embeddings_scaled = scaler.fit_transform(embeddings)\n",
        "\n",
        "        pca = PCA(n_components=n_components)\n",
        "        embeddings_processed = pca.fit_transform(embeddings_scaled)\n",
        "\n",
        "        return embeddings_processed, pca, scaler\n",
        "\n",
        "    return embeddings, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV94lQngUVbO",
        "outputId": "2e6736f9-2ad6-4f55-f6b9-80cce924ddc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|â–Ž         | 2/54 [00:30<12:50, 14.81s/it]"
          ]
        }
      ],
      "source": [
        "def main(csv_path, output_path):\n",
        "    texts = load_data(csv_path)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    model = SimCSEModel(\n",
        "        model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "        pooling='mean',\n",
        "        projection_dim=384,\n",
        "        temperature=0.05\n",
        "    )\n",
        "\n",
        "    train_texts, val_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_dataset = ImprovedSimCSEDataset(train_texts, tokenizer)\n",
        "    val_dataset = ImprovedSimCSEDataset(val_texts, tokenizer)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    trainer = SimCSETrainer(model, tokenizer)\n",
        "\n",
        "    trainer.train(train_dataloader, val_dataloader, epochs=5)\n",
        "\n",
        "    embeddings = trainer.generate_embeddings(texts)\n",
        "\n",
        "    embeddings_processed, pca, scaler = post_process_embeddings(embeddings)\n",
        "\n",
        "    np.save(output_path, embeddings_processed)\n",
        "\n",
        "    return embeddings_processed, trainer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_path = \"bbc_encoded.csv\"\n",
        "    output_path = \"simcse_embeddings.npy\"\n",
        "\n",
        "    embeddings, trainer = main(csv_path, output_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
